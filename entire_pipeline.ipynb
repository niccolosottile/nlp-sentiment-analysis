{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading reviews\n",
    "\n",
    "Reads the reviews from each folder, then joins them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_and_preprocess import read_reviews, preprocess_reviews\n",
    "\n",
    "# Read positive and negative reviews\n",
    "pos_reviews = read_reviews('../data/pos') \n",
    "neg_reviews = read_reviews('../data/neg') \n",
    "\n",
    "# Merge them in single dictionary\n",
    "all_reviews = {}\n",
    "all_reviews.update(pos_reviews)\n",
    "all_reviews.update(neg_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Optional functions to reduce dimensionality of feature space (for BoW and TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def select_top_features_tfidf(tfidf, top_n=10000):\n",
    "    \"\"\"Decreases the feature space by selecting the top_n features with higher tf-idf scores.\"\"\"\n",
    "    # Calculate the average tf-idf score for each feature\n",
    "    avg_scores = np.mean(tfidf, axis=0)\n",
    "\n",
    "    # Get indices of top features\n",
    "    top_indices = np.argsort(avg_scores)[::-1][:top_n]\n",
    "    \n",
    "    return tfidf[:, top_indices]\n",
    "\n",
    "def select_top_features_bow(bow, top_n=10000):\n",
    "    \"\"\"Decreases the feature space by selecting the top_n features with higher counts.\"\"\"\n",
    "    # Sum feature occurrences\n",
    "    sums = np.array(bow.sum(axis=0)).ravel()\n",
    "\n",
    "    # Get indices of top features\n",
    "    top_indices = np.argsort(sums)[::-1][:top_n]\n",
    "\n",
    "    # Select only columns corresponding to top features\n",
    "    return bow[:, top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature set generation\n",
    "\n",
    "Generates features sets using fitting or transform options, applies preprocessing, feature generation, and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from tfidf import calcualate_tfs, calculate_idfs, calculate_tfidfs\n",
    "\n",
    "# Global variables to store fitted data\n",
    "fitted_bow_vectorizer = None\n",
    "fitted_idfs = None\n",
    "fitted_term_to_index = None\n",
    "\n",
    "def generate_features(contents, params, fit = True):\n",
    "    \"\"\"Generates a set of features using preprocessing and tf-idf or BoW, either fits or not.\"\"\"\n",
    "    global fitted_bow_vectorizer, fitted_idfs, fitted_term_to_index\n",
    "\n",
    "    a_tfidf = params[-1]\n",
    "    a_bow = params[-2]\n",
    "\n",
    "    preprocessed_contents = preprocess_reviews(contents, *params[:-2])\n",
    "\n",
    "    # Choose whether to use BoW or tf-idf\n",
    "    if a_bow:\n",
    "        # Calculate counts for each review\n",
    "        bow = [Counter(content.split()) for content in preprocessed_contents]\n",
    "\n",
    "        if fit:\n",
    "            # Fits feature space\n",
    "            fitted_bow_vectorizer = DictVectorizer()\n",
    "            fitted_bow_vectorizer.fit(bow)\n",
    "        \n",
    "        # Transform using fitted (just now or previously) BoW\n",
    "        bow = fitted_bow_vectorizer.transform(bow)\n",
    "\n",
    "        # Select top_n features\n",
    "        sparse_vector = bow #select_top_features_bow(bow, 50000)\n",
    "\n",
    "    elif a_tfidf:\n",
    "        if fit: \n",
    "            # Calculate tfs over each review and whole vocabulary\n",
    "            doc_terms, doc_count = calcualate_tfs(preprocessed_contents)\n",
    "\n",
    "            # Fit idfs\n",
    "            num_docs = len(preprocessed_contents)\n",
    "            fitted_idfs = calculate_idfs(doc_count, num_docs)\n",
    "\n",
    "            # Create a sorted list of all unique terms\n",
    "            all_terms = sorted(set(term for terms in doc_terms.values() for term in terms))\n",
    "\n",
    "            # Map from terms to column indices\n",
    "            fitted_term_to_index = {term: index for index, term in enumerate(all_terms)}  \n",
    "            \n",
    "        # Transform using fitted (just now or previously) tf-idf\n",
    "        tfidfs = calculate_tfidfs(preprocessed_contents, fitted_idfs, fitted_term_to_index)\n",
    "\n",
    "        # Select top_n features\n",
    "        sparse_vector = tfidfs #select_top_features_tfidf(tfidfs, 50000)\n",
    "\n",
    "    return sparse_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation\n",
    "\n",
    "#### Uses Implemented NB, Multinomial NB, SGD Logistic Regression, and Linear SVC\n",
    "\n",
    "Applies training and evalation on each model for each feature set generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of features \n",
    "# params: remove stopwords, remove punctuation, stemming, lemmatization, BoW, TF-IDF\n",
    "params_1 = [False, True, False, False, False, True] # 1: punctuation, TFIDF\n",
    "params_2 = [True, True, False, False, False, True] # 2: stopwords, punctuation, TFIDF\n",
    "params_3 = [True, True, False, False, True, False] # 3: stopwords, punctuation, BoW \n",
    "params_4 = [True, True, True, False, False, True] # 4: stopwords, punctuation, stemming, TFIDF \n",
    "params_5 = [True, True, False, True, True, False] # 5: stopwords, punctuation, lemmatization, BoW \n",
    "all_params = [params_1, params_2, params_3, params_4, params_5]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting reviews and labels up\n",
    "X = np.array([review['content'] for review in all_reviews.values()])\n",
    "y = np.array([review['label'] for review in all_reviews.values()])\n",
    "\n",
    "# Separate out the train, dev, and test sets\n",
    "X_train_dev, X_test, y_train_dev, y_test = train_test_split(X, y, test_size = 0.15, stratify = y, random_state = 31)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_dev, y_train_dev, test_size = 0.15 / 0.85, stratify = y_train_dev, random_state = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from naive_bayes import ImplementedNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for i, params in enumerate(all_params, start=1):\n",
    "    # Generating features for set\n",
    "    train_features = generate_features(X_train, params)\n",
    "\n",
    "    # Generate features for dev and test sets without fitting (applies feature space of train set)\n",
    "    dev_features = generate_features(X_dev, params, fit = False)\n",
    "    test_features = generate_features(X_test, params, fit = False)\n",
    "\n",
    "    print(f\"\\nResults for feature set {i}:\")\n",
    "    rows, columns = train_features.shape\n",
    "    print(f\"Feature set size: {columns}\\n\")\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    # Implemented Naive Bayes classifier\n",
    "    print(\"Implemented Naive Bayes Classifier\")\n",
    "    classifier = ImplementedNB()\n",
    "    classifier.fit(train_features, y_train)\n",
    "\n",
    "    # Evaluate on the development set\n",
    "    y_dev_pred = classifier.predict(dev_features)\n",
    "    dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n",
    "    print(\"Development set accuracy:\", dev_accuracy)\n",
    "    print(\"Development set classification report:\\n\", classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    # Multinomial Naive Bayes classifier\n",
    "    print(\"Multinomial Naive Bayes Classifier\")\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(train_features, y_train)\n",
    "\n",
    "    # Evaluate on the development set\n",
    "    y_dev_pred = classifier.predict(dev_features)\n",
    "    dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n",
    "    print(\"Development set accuracy:\", dev_accuracy)\n",
    "    print(\"Development set classification report:\\n\", classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    # SGD Logistic Regression\n",
    "    print(\"SGD Logistic Regression\")\n",
    "    sgd_logistic = SGDClassifier(loss='log_loss', random_state=31)\n",
    "    sgd_logistic.fit(train_features, y_train)\n",
    "\n",
    "    # Evaluate on the development set\n",
    "    y_dev_pred_sgd = sgd_logistic.predict(dev_features)\n",
    "    dev_accuracy_sgd = accuracy_score(y_dev, y_dev_pred_sgd)\n",
    "    print(\"Development set accuracy:\", dev_accuracy_sgd)\n",
    "    print(\"Development set classification report:\\n\", classification_report(y_dev, y_dev_pred_sgd))\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    # SVM Classifier\n",
    "    print(\"Linear SVC Classifier\")\n",
    "    svm_classifier = LinearSVC(dual=False, random_state=31)\n",
    "    svm_classifier.fit(train_features, y_train)\n",
    "\n",
    "    # Evaluate on the development set\n",
    "    y_dev_pred_svm = svm_classifier.predict(dev_features)\n",
    "    dev_accuracy_svm = accuracy_score(y_dev, y_dev_pred_svm)\n",
    "    print(\"Development set accuracy:\", dev_accuracy_svm)\n",
    "    print(\"Development set classification report:\\n\", classification_report(y_dev, y_dev_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "#### Performs hyperparameter tuning using grid search on Linear SVC model. \n",
    "\n",
    "Using hyperparameters:\n",
    "\n",
    "* C -> smaller C indicates stronger regularisation\n",
    "\n",
    "* tol -> tolerance for stopping criteria\n",
    "\n",
    "* max_iter -> maximum number of iterations allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter optimisation on the dev set \n",
    "# (using chosen best combination of method, and feature set)\n",
    "# Linear SVC Classifier on feature set 1 (accuracy 0.858)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "C_values = [0.01, 0.1, 1]\n",
    "tol_values = [1e-4, 1e-3, 1e-2]\n",
    "max_iter_values = [1000, 5000, 10000]\n",
    "\n",
    "# Generating features for train, dev, and test sets\n",
    "train_features = generate_features(X_train, params_1)\n",
    "dev_features = generate_features(X_dev, params_1, fit=False)\n",
    "test_features = generate_features(X_test, params_1, fit=False)\n",
    "\n",
    "# Initialize variables to store the best hyperparameters and the corresponding accuracy\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for C in C_values:\n",
    "    for tol in tol_values:\n",
    "        for max_iter in max_iter_values:\n",
    "            # Create and train the LinearSVC model\n",
    "            svm_classifier = LinearSVC(C=C, tol=tol, max_iter=max_iter, dual=False, random_state=31)\n",
    "            svm_classifier.fit(train_features, y_train)\n",
    "\n",
    "            # Evaluate on the development set\n",
    "            y_dev_pred = svm_classifier.predict(dev_features)\n",
    "            dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n",
    "\n",
    "            # Update the best hyperparameters if current model is better\n",
    "            if dev_accuracy > best_accuracy:\n",
    "                best_accuracy = dev_accuracy\n",
    "                best_params = {'C': C, 'tol': tol, 'max_iter': max_iter}\n",
    "\n",
    "# Train a new model using the best hyperparameters\n",
    "best_svm = LinearSVC(**best_params, dual=False, random_state=31)\n",
    "best_svm.fit(train_features, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_test_pred = best_svm.predict(test_features)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(\"Best hyperparameters (based on development set):\", best_params)\n",
    "print(\"Test set accuracy with best hyperparameters:\", test_accuracy)\n",
    "print(\"Test set classification report:\\n\", test_report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46abff27e78d9f02caa1fe740b8268973b81923f0dc90df308f158af5dbc826d"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
