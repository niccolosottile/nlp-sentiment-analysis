{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_and_preprocess import read_reviews\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Read positive and negative reviews\n",
    "pos_reviews = read_reviews('../data/pos') \n",
    "neg_reviews = read_reviews('../data/neg') \n",
    "\n",
    "# Merge them in single dictionary\n",
    "all_reviews = {}\n",
    "all_reviews.update(pos_reviews)\n",
    "all_reviews.update(neg_reviews)\n",
    "\n",
    "# Splitting reviews and labels up\n",
    "X = np.array([review['content'] for review in all_reviews.values()])\n",
    "y = np.array([review['label'] for review in all_reviews.values()])\n",
    "\n",
    "# Separate out the train, dev, and test sets\n",
    "X_train_dev, X_test, y_train_dev, y_test = train_test_split(X, y, test_size = 0.15, stratify = y, random_state = 31)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_dev, y_train_dev, test_size = 0.15 / 0.85, stratify = y_train_dev, random_state = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Implementing BERT\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Using the BERT tokenizer for the base model \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # uncased\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased') # cased\n",
    "\n",
    "def preprocess_reviews(contents):\n",
    "    \"\"\"Preprocess contents for BERT model.\"\"\"\n",
    "    preprocessed_contents = tokenizer(contents, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    return preprocessed_contents\n",
    "\n",
    "# Apply tokenization to the train, dev, and test splits\n",
    "X_train_tokenized = preprocess_reviews(X_train.tolist())\n",
    "X_dev_tokenized = preprocess_reviews(X_dev.tolist())\n",
    "X_test_tokenized = preprocess_reviews(X_test.tolist())\n",
    "\n",
    "# Create a Dataset class for Trainer to use\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve each encoding type: input_ids, attention_mask, \n",
    "        # slicing the tensor to include only idx of given review\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        # Named labels since Trainer API expects it (could be multi-label classification)\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create Datasets objects for the Trainer\n",
    "train_dataset = ReviewsDataset(X_train_tokenized, torch.tensor(y_train))\n",
    "dev_dataset = ReviewsDataset(X_dev_tokenized, torch.tensor(y_dev))\n",
    "test_dataset = ReviewsDataset(X_test_tokenized, torch.tensor(y_test))\n",
    "\n",
    "# Using the base BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # uncased\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-cased') # cased\n",
    "\n",
    "# Defining arguments to provide to the Trainer\n",
    "arguments = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1, # gradually increases learning rate after warmup\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True, # loads the best model at the end of training\n",
    "    evaluation_strategy=\"epoch\", # evaluates at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # save at the end of each epoch\n",
    ")\n",
    "\n",
    "# Creating the Trainer by loading model, arguments, and train and eval sets\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=arguments, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "# Train the BERT model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the BERT model (choosing best from dev set)\n",
    "best_trainer = trainer\n",
    "best_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46abff27e78d9f02caa1fe740b8268973b81923f0dc90df308f158af5dbc826d"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
